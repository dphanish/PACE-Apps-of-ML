{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:24pt; text-decoration:underline; font-weight:bold; color:#003057; text-align:center\">\n",
    "    PACE - Applications of Machine Learning\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "        <a href = \"mailto: dnagendra3@gatech.edu\"><b>Deepa Phanish, PhD</b></a> : <a href = \"https://pace.gatech.edu\" target = \"_blank\"><b>PACE, Georgia Tech</b></a> <br>\n",
    "    <a href = \"mailto: ajezghani3@gatech.edu\"><b>Aaron Jezghani, PhD</b></a> : <a href = \"https://pace.gatech.edu\" target = \"_blank\"><b>PACE, Georgia Tech</b></a> <br>\n",
    "    <a href = \"mailto: chris_blanton@ncsu.edu\"><b>Chris Blanton, PhD</b></a> : <a href = \"https://www.lib.ncsu.edu\" target= \"_blank\"><b>Research Consulting, NCSU</b></a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>Introduction</b></u></font>\n",
    "\n",
    "<br>\n",
    "<center><font size=4><i>[Machine Learning is a] field of study that gives computers the ability to learn without being explicitly programmed.</i> <br> --- Arthur Samuel, 1959</font></center>\n",
    "\n",
    "Machine learning has existed for decades; however, until recently, computing power and data storage were too limited to allow machines to solve many problems in the field effectively. Advances in speed and density have taken machine learning from an abstract idea to the forefront of scientific research across many domains, including:\n",
    "- Bioinformatics\n",
    "- Molecular dynamics\n",
    "- Astrophysics\n",
    "- Signal processing\n",
    "- Health data analytics\n",
    "- Finance and marketing\n",
    "- Urban planning\n",
    "- ...and many more\n",
    "\n",
    "The volume of data being generated and made available for research is rapidly increasing, and while the technology is constantly being reconsidered to keep up, the challenge of meaningfully utilizing it is becoming ever more present. Even if we ignore the challenges of human bias, the efficiency of humans in reviewing data remains \n",
    "\n",
    "<font size=4 color=B3A369><b>Spam Filters: The Traditional Way</b></font>\n",
    "\n",
    "1. We want to start by identifying some common features in spam emails; these could be:\n",
    "    - phrases (\"4U\", \"one simple trick\", \"aliens\", \"lottery\"), \n",
    "    - questionable domains (\"google.asdqwkjf92.ohno\", \"definitely-not-stealing-your-info.org\"),\n",
    "    - mismatches in sender's name/email (\"Mary Smith from aaron.aaron@itsascam.net\"),\n",
    "    - etc.\n",
    "2. We would write some rules to capture these bits\n",
    "    - You may have already done this for your school/work emails to sort by topic!\n",
    "3. We then execute the email rules to test their validity\n",
    "    - We need to identify correctness, including false positives (good emails mislabeled as spam) and false negatives (spam emails that were missed by our rules)\n",
    "4. We update our rules accordingly, and repeat until we're satisfied.\n",
    "    - Note that as new approaches are deployed, we have to identify and define new features to update our filter.\n",
    "\n",
    "<img src=\"image/traditional-programming.png\" alt=\"Traditional Programming\" width=\"600\"> \n",
    "\n",
    "<font size=4 color=B3A369><b>Spam Filters: Using Machine Learning</b></font>\n",
    "\n",
    "1. We need a reasonably large collection of emails that have been labeled as spam or not spam.\n",
    "    - The dataset may have defined characteristics (sender, message length, domain, etc.), or it may not, in which case we have to define our own.\n",
    "    - Additionally, we can always define additional fields from existing data through a process known as **feature engineering** (e.g., _x_ and _y_ coordinates might be better presented as polar coordinates for data centered around some location).\n",
    "2. We choose an algorithm that can consider the input characteristics and the categorization to determine which emails are spam and which are not.\n",
    "    - There are numerous existing algorithms (linear/logistic regression, neural networks, nearest neighbor, etc.) that can be modified, or a new algorithm can be defined (this is a very active research area after all!).\n",
    "3. We divide the dataset into training and test subsets, typically through some form of random selection.\n",
    "    - Different algorithms perform differently depending on the data, so we want to ascertain our model's efficacy before deploying it in the wild.\n",
    "    - Because of the underlying statistcs used in ML, it can often be helpful to explore resampling in an effort to assess the validity of our model.\n",
    "4. We tune our algorithm until we reach the desired level of accuracy, and then we deploy the model in the real world.\n",
    "    - Unlike the traditional method, machine learning has the ability to adapt to novel spam data - the model might need retraining, but we can in theory still use the same algorithm (this is what you're doing when you provide feedback on applications like Google Maps).\n",
    "\n",
    "<img src=\"image/machine_learning.png\" alt=\"Machine Learning\" width=\"600\"> \n",
    "\n",
    "<font size=5 color=B3A369><u><b>Using the Correct Tools...Correctly</b></u></font>\n",
    "\n",
    "There's often a difficult progression with ML projects from proof-of-concept to large-scale application, and a little foresight can reduce headaches significantly. Fortunately, framework such as TensorFlow and PyTorch significantly ease the transition from CPU to GPU, to the point where developing directly for GPU training is accessible. However, issues with scaling, either in terms of data or process distribution, can often manifest as one increases the scope of their project. Additionally, hardware-specific optimizations may be missed as code is migrated to new hardware, especially if outdated versions of computational libraries are used.\n",
    "\n",
    "Based on the hardware you've chosen to use, it's always worthwhile to explore the vendor's recommended settings. Since we'll be using Intel CPUs, we can take a look at [their recommendations](https://www.intel.com/content/www/us/en/developer/articles/technical/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference.html):\n",
    "\n",
    "- KMP_AFFINITY=granularity=fine,verbose,compact,1,0 <font color=008080><em>#Bind OpenMP threads to physical cores</em></font>\n",
    "- TF_ENABLE_ONEDNN_OPTS=1 <font color=008080><em>#Enable Intel® oneAPI Deep Neural Network Library capabilities</em></font>\n",
    "- OMP_NUM_THREADS=${PBS_NP} <font color=008080><em>#One thread per physical core</em></font>\n",
    "- KMP_BLOCKTIME=0 <font color=008080><em>#Sets time that threads wait before sleeping</em></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_AFFINITY']=\"granularity=fine,verbose,compact\" #No hyper-threading\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']=\"1\" #OneDNN CPU optimizations\n",
    "os.environ['OMP_NUM_THREADS']=os.getenv('PBS_NP') #Single hardware thread per core\n",
    "os.environ['KMP_BLOCKTIME']=\"0\" #empirically test the correct value - >0 for non-OMPthreaded code embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>Practical Example: ML in Medicine</b></u></font>\n",
    "\n",
    "When developing a machine learning workflow, it can be tempting to focus solely on the machine learning algorithm and model refinement. However, before that aspect can be considered, there are other challenges:\n",
    "- Where do we get the data?\n",
    "- Is the data formatted appropriately for the system?\n",
    "- What framework/hardware will be used?\n",
    "\n",
    "To explore these issues, we can use a standard training example from the community: automated breast cancer detection. As you may be aware, breast cancer is one of the most common cancers among women worldwide. Early diagnosis of breast cancer can greatly increase the outlook for patients, but accurate diagnosis can be a challenge as it requires expert analysis, and thus areas lacking in experts can be greatly affected.\n",
    "\n",
    "The Wisconsin breast cancer dataset consists of 30 parameters obtained via analysis of fine needle aspiration (FNA) biopsy of breast masses. The data describes characteristics of the cell nuclei in the image. This dataset has been previously studied in several papers, including [Breast Cancer Detection with Reduced Feature Set](https://www.hindawi.com/journals/cmmm/2015/265138/). Because each mass is labeled as benign or malignant, we can use the data to explore the application of machine learning techniques and gain insights into the viability of ML for real-world applications such as this.\n",
    "\n",
    "<font size=4 color=B3A369><b>Our Plan of Action</b></font>\n",
    "\n",
    "In this workshop, we will look at an example workflow on one of PACE's instructional clusters using CPUs for analysis. In short, we will use our labeled dataset of thirty features to train a classifier that will attempt to label new data as either benign or malignant. The steps we will take are as follows:\n",
    "1. Import the necessary libraries to explore/analyze our data and develop our model.\n",
    "2. Acquire our data and transform it to a useable form.\n",
    "3. Explore our data and garner any initial insights that might help us in our efforts.\n",
    "4. Prepare the data for training.\n",
    "5. Split the data into training and test subsets. !! This needs to be done before data prep to avoid information leakage\n",
    "6. Pick our ML algorithm and train our model.\n",
    "7. Test and evaluate our model to further explore the training process.\n",
    "\n",
    "<font size=5 color=B3A369><u><b>1. Import Libraries</b></u></font>\n",
    "\n",
    "There are numerous libraries that can be utilized in an ML project - we'll try to touch on several in this workshop to provide broader familiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.4\r\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import sklearn\n",
    "#show images inline with code block\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it is helpful to check versions of packages to verify capability/compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.1\n",
      "Eager execution is: True\n",
      "Keras version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version: {}\".format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eager execution** is enabled by default (and was meant to tackle a big issue in TFv1). It:\n",
    "- evaluates operations immediately\n",
    "- returns actual values rather than computational graphs to be run later\n",
    "- calculates the values of tensors as they occur\n",
    "\n",
    "Broadly speaking, it's meant to make things simpler and more accessible for beginners.\n",
    "\n",
    "*However*, disabling this feature provides an opportunity for more optimization, as you can extract tensor computations and build a more effecient graph before proceeding. Thus, more advanced users may desire to run in this mode instead - for today, though, we'll keep it enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>2. Acquire/Transform Data</b></u></font>\n",
    "\n",
    "There are numerous sources for ML training sets, including directly from the Python libraries themselves. Picking one from the ML framework you're using has the advantage that it is usually formatted correctly, but since we want to explore the data transformation component of our workflow, we'll take our dataset from the SciKit-Learn datasets. In real-world, you may need to do plenty of data clean-up before getting here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer # Loading the breast cancer from a standard datasource within SciKit\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data to get a better understanding of how it is formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the data uses human-readable characters, it's not formatted for easy reading by a human. We can manipulate it to change that!\n",
    "\n",
    "First, let's start by getting the names of the fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this Python dictionary, we can focus on a single component of the dataset rather than dumping a block of information. For example, we can read the data description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to develop a classifier, ultimately we need to explore the labels, or targets, for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "(569,)\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "print(cancer['target'])\n",
    "print(cancer['target'].shape) \n",
    "print(cancer['target'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the target data is presented as a binary encoding (either 0 or 1). If we want to know which value maps to which label, we can use the \"target_names\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print(cancer['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the array is 0-indexed, that means a value of '0' maps to 'malignant' and a value of '1' corresponds to 'benign'. \n",
    "\n",
    "Carrying on, the features of the set can be found by looking at the 'features_names' entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print(cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=B3A369><b>Pandas Dataframes</b></font>\n",
    "\n",
    "Since the data exhibits a fair amount of variety, we want to store it into an appropriate object. Pandas dataframes offer an excellent solution - they are data structures that provide labeled axes for heterogeneous data types, so they do exactly what we want! To convert our dataset to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer = pd.DataFrame(np.c_[cancer['data'],cancer['target']],columns=np.append(cancer['feature_names'],['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, TensorFlow does not allow spaces in feature names, so we'll have to fix that. This can be accomplished by looping over our dataset, replacing the spaces with a suitable character (e.g. an underscore), and updating our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n",
      "       'mean_smoothness', 'mean_compactness', 'mean_concavity',\n",
      "       'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension',\n",
      "       'radius_error', 'texture_error', 'perimeter_error', 'area_error',\n",
      "       'smoothness_error', 'compactness_error', 'concavity_error',\n",
      "       'concave_points_error', 'symmetry_error', 'fractal_dimension_error',\n",
      "       'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area',\n",
      "       'worst_smoothness', 'worst_compactness', 'worst_concavity',\n",
      "       'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension',\n",
      "       'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for key in df_cancer.keys():\n",
    "    newkey = key.replace(\" \", \"_\")\n",
    "    df_cancer.rename(index=str,columns={key:newkey},inplace=True)\n",
    "print(df_cancer.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the nice functionality of a dataframe to look at the beginning of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean_fractal_dimension  ...  worst_texture  worst_perimeter  worst_area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst_smoothness  worst_compactness  worst_concavity  worst_concave_points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst_symmetry  worst_fractal_dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or the end of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean_fractal_dimension  ...  worst_texture  worst_perimeter  worst_area  \\\n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst_concave_points  worst_symmetry  worst_fractal_dimension  target  \n",
       "564                0.2216          0.2060                  0.07115     0.0  \n",
       "565                0.1628          0.2572                  0.06637     0.0  \n",
       "566                0.1418          0.2218                  0.07820     0.0  \n",
       "567                0.2650          0.4087                  0.12400     0.0  \n",
       "568                0.0000          0.2871                  0.07039     1.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're feeling wild, we can even request more than 5 rows at a time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.22</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.09429</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.1700</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.92</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.31740</td>\n",
       "      <td>0.14740</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.6599</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "562        15.22         30.62          103.40      716.9          0.10480   \n",
       "563        20.92         25.09          143.00     1347.0          0.10990   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "562           0.20870         0.25500              0.09429         0.2128   \n",
       "563           0.22360         0.31740              0.14740         0.2149   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean_fractal_dimension  ...  worst_texture  worst_perimeter  worst_area  \\\n",
       "562                 0.07152  ...          42.79           128.70       915.0   \n",
       "563                 0.06879  ...          29.41           179.10      1819.0   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "562           0.14170            0.79170           1.1700   \n",
       "563           0.14070            0.41860           0.6599   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst_concave_points  worst_symmetry  worst_fractal_dimension  target  \n",
       "562                0.2356          0.4089                  0.14090     0.0  \n",
       "563                0.2542          0.2929                  0.09873     0.0  \n",
       "564                0.2216          0.2060                  0.07115     0.0  \n",
       "565                0.1628          0.2572                  0.06637     0.0  \n",
       "566                0.1418          0.2218                  0.07820     0.0  \n",
       "567                0.2650          0.4087                  0.12400     0.0  \n",
       "568                0.0000          0.2871                  0.07039     1.0  \n",
       "\n",
       "[7 rows x 31 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=B3A369><b>Feature Scaling</b></font>\n",
    "\n",
    "One thing that may immediately jump out is the variation in scale of the values for the different features. This can cause a few issues in our analysis if we're not careful:\n",
    "1. Some algorithms may not function when the scale of features is wildly different. For example, if a Euclidean distance is calculated, the larger feature may completely dominate the calculation.\n",
    "2. Gradient descent can converge more quickly if features are normalized, which can aid in training time.\n",
    "3. If regularization is used as part of the loss function, feature scaling is important to ensure that coefficients are penalized properly.\n",
    "\n",
    "Scikit-learn has built-in functionality that can perform our scaling and put all values in the range 0 to 1. Be careful not to scale the target too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
      "0     0.521037      0.022658        0.545989   0.363733         0.593753   \n",
      "1     0.643144      0.272574        0.615783   0.501591         0.289880   \n",
      "2     0.601496      0.390260        0.595743   0.449417         0.514309   \n",
      "3     0.210090      0.360839        0.233501   0.102906         0.811321   \n",
      "4     0.629893      0.156578        0.630986   0.489290         0.430351   \n",
      "\n",
      "   mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
      "0          0.792037        0.703140             0.731113       0.686364   \n",
      "1          0.181768        0.203608             0.348757       0.379798   \n",
      "2          0.431017        0.462512             0.635686       0.509596   \n",
      "3          0.811361        0.565604             0.522863       0.776263   \n",
      "4          0.347893        0.463918             0.518390       0.378283   \n",
      "\n",
      "   mean_fractal_dimension  ...  worst_texture  worst_perimeter  worst_area  \\\n",
      "0                0.605518  ...       0.141525         0.668310    0.450698   \n",
      "1                0.141323  ...       0.303571         0.539818    0.435214   \n",
      "2                0.211247  ...       0.360075         0.508442    0.374508   \n",
      "3                1.000000  ...       0.385928         0.241347    0.094008   \n",
      "4                0.186816  ...       0.123934         0.506948    0.341575   \n",
      "\n",
      "   worst_smoothness  worst_compactness  worst_concavity  worst_concave_points  \\\n",
      "0          0.601136           0.619292         0.568610              0.912027   \n",
      "1          0.347553           0.154563         0.192971              0.639175   \n",
      "2          0.483590           0.385375         0.359744              0.835052   \n",
      "3          0.915472           0.814012         0.548642              0.884880   \n",
      "4          0.437364           0.172415         0.319489              0.558419   \n",
      "\n",
      "   worst_symmetry  worst_fractal_dimension  target  \n",
      "0        0.598462                 0.418864     0.0  \n",
      "1        0.233590                 0.222878     0.0  \n",
      "2        0.403706                 0.213433     0.0  \n",
      "3        1.000000                 0.773711     0.0  \n",
      "4        0.157500                 0.142595     0.0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "     mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
      "564     0.690000      0.428813        0.678668   0.566490         0.526948   \n",
      "565     0.622320      0.626987        0.604036   0.474019         0.407782   \n",
      "566     0.455251      0.621238        0.445788   0.303118         0.288165   \n",
      "567     0.644564      0.663510        0.665538   0.475716         0.588336   \n",
      "568     0.036869      0.501522        0.028540   0.015907         0.000000   \n",
      "\n",
      "     mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
      "564          0.296055        0.571462             0.690358       0.336364   \n",
      "565          0.257714        0.337395             0.486630       0.349495   \n",
      "566          0.254340        0.216753             0.263519       0.267677   \n",
      "567          0.790197        0.823336             0.755467       0.675253   \n",
      "568          0.074351        0.000000             0.000000       0.266162   \n",
      "\n",
      "     mean_fractal_dimension  ...  worst_texture  worst_perimeter  worst_area  \\\n",
      "564                0.132056  ...       0.383262         0.576174    0.452664   \n",
      "565                0.113100  ...       0.699094         0.520892    0.379915   \n",
      "566                0.137321  ...       0.589019         0.379949    0.230731   \n",
      "567                0.425442  ...       0.730277         0.668310    0.402035   \n",
      "568                0.187026  ...       0.489072         0.043578    0.020497   \n",
      "\n",
      "     worst_smoothness  worst_compactness  worst_concavity  \\\n",
      "564          0.461137           0.178527         0.328035   \n",
      "565          0.300007           0.159997         0.256789   \n",
      "566          0.282177           0.273705         0.271805   \n",
      "567          0.619626           0.815758         0.749760   \n",
      "568          0.124084           0.036043         0.000000   \n",
      "\n",
      "     worst_concave_points  worst_symmetry  worst_fractal_dimension  target  \n",
      "564              0.761512        0.097575                 0.105667     0.0  \n",
      "565              0.559450        0.198502                 0.074315     0.0  \n",
      "566              0.487285        0.128721                 0.151909     0.0  \n",
      "567              0.910653        0.497142                 0.452315     0.0  \n",
      "568              0.000000        0.257441                 0.100682     1.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = df_cancer.copy()\n",
    "df_scaled[['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n",
    "       'mean_smoothness', 'mean_compactness', 'mean_concavity',\n",
    "       'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension',\n",
    "       'radius_error', 'texture_error', 'perimeter_error', 'area_error',\n",
    "       'smoothness_error', 'compactness_error', 'concavity_error',\n",
    "       'concave_points_error', 'symmetry_error', 'fractal_dimension_error',\n",
    "       'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area',\n",
    "       'worst_smoothness', 'worst_compactness', 'worst_concavity',\n",
    "       'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension']]=  scaler.fit_transform(df_scaled[['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n",
    "       'mean_smoothness', 'mean_compactness', 'mean_concavity',\n",
    "       'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension',\n",
    "       'radius_error', 'texture_error', 'perimeter_error', 'area_error',\n",
    "       'smoothness_error', 'compactness_error', 'concavity_error',\n",
    "       'concave_points_error', 'symmetry_error', 'fractal_dimension_error',\n",
    "       'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area',\n",
    "       'worst_smoothness', 'worst_compactness', 'worst_concavity',\n",
    "       'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension']])\n",
    "print(df_scaled.head())\n",
    "print(df_scaled.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>3. Explore Data</b></u></font>\n",
    "\n",
    "Often it is assumed that with all the data, everything can be known. As it turns out, though, gathering the data isn't the only challenge of this approach; redundancy in the data and too many features can lead to inefficiencies in training. For example, in our dataset are there multiple descriptions of the same property that don't provide a additional insights? Or perhaps, are there any data points that seem purely superfluous?\n",
    "\n",
    "The challenge, of course, is how can we meaningfully pare down our data. Since we want to determine the target with our classifier, we can start by exploring how strongly correlated each feature is with the target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_scaled.corr()['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, it helps if we organize our list, so let's start there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.corr()['target'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can give us insights into the impact of any one feature in terms of determining the target value, but lacks any information about data redundancies that may exist between different features. \n",
    "\n",
    "<font size=4 color=B3A369><b>Dataset Visualization</b></font>\n",
    "\n",
    "To explore the relationships between variables, we can utilize a correlation matrix heatmap, where the color indicates how strongly correlated each pair of features is. Values close to -1 show a strong negative correlation (one variable increases as the other decreases), while values close to +1 show a strong positive correlation (both increase or decrease together).\n",
    "\n",
    "While we can use other libraries to generate this heatmap, the Seaborn library provides a very simple, dataframe-compatible function to achieve our goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_scaled.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the default size can be difficult to view. Let's try to make that a little better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_scaled.corr(),annot=True) # This is because of an issue in matplotlib. \n",
    "bottom, top = ax.get_ylim() \n",
    "ax.set_ylim(bottom+0.5, top-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to use all the features in a haphazard way. In our case, we see several parameters, such as the size of the cells to have multiple types of measurements. It is often advanatageous to try to minimize the number of features because of this and computational expense. \n",
    "\n",
    "Thinking about this in a mathematical sense, the features do not necessarily form a orthogonal basis set. This can lead to degenerate answers which may complicate the optimization process and either lead to a local extrema or failure of convergence. **This is in general terms of optimization, not strictly ML terms.** \n",
    "\n",
    "As related and practical matter, the large the feature set, the more expensive the calcuation is. By reducing the number of features, we try to increase the \"siginal-to-noise\" while decreasing the computational expense. \n",
    "\n",
    "In our case, we will use the mean parameters for a starting point because it reduces the number of features to 5. Inuitively, mean values tend to be a good choice for measuring trends.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_scaled, vars=['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness','mean_concave_points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Seaborn to color-code each datapoint based on the target value. This can help identify feature relationships that are the most indicative of a certain target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_scaled,hue='target', vars=['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness','mean_concave_points'])\n",
    "# Below is to allow the legend to use words instead of numbers. \n",
    "handles = g._legend_data.values()\n",
    "labels = ['Malignant','Benign'] \n",
    "g._legend.remove()\n",
    "g.fig.legend(handles=handles,labels=labels, loc='center right',ncol=1)\n",
    "g.fig.subplots_adjust(top=0.92,bottom=0.08,right=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we might find it helpful to understand just how many values we have for each target label. A significant disparity can unintentionally bias our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df_scaled['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>4. Prepare Data</b></u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above, it seems fair to conclude that we have several values that are highly correlated, and that if we limit ourselves to just the mean values, we can reasonably represent the available data while maintaining efficiency (Disclaimer: this is just illustrative of a valid thought process, which may or may not actually be the optimal path forward with this dataset).\n",
    "\n",
    "In preparation of our application, we can define the features and labels to use in our ML classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['mean_radius','mean_concave_points','mean_perimeter','mean_area','mean_smoothness','mean_concavity','mean_texture']\n",
    "labels=['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to shuffle our dataset. It's not uncommon to be provided a dataset that is sorted, which can bias results, so randomizing the order is usually a good first step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_data = df_scaled.reindex(np.random.permutation(df_scaled.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we didn't reindex our dataframe, we can see the original row labels and confirm that the order is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>5. Split Data</b></u></font>\n",
    "\n",
    "Next, we need to define our training and test datasets. The trick here is to designate some fraction of our total dataset to be used for training, and then use the remainder to validate our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = len(randomized_data)\n",
    "training_set_size_portion = 0.8\n",
    "training_set_size = int(total_records*training_set_size_portion)\n",
    "test_set_size = total_records - training_set_size\n",
    "print(total_records,training_set_size,test_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate our test by using the tail function on our randomized_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the testing features and labels\n",
    "testing_features = randomized_data.tail(test_set_size)[features].copy()\n",
    "testing_labels = randomized_data.tail(test_set_size)[labels].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can verify the content of the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and see that the indices for the targets match those of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build our training set from the other portion of randomized_data and confirm the same about its indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = randomized_data.head(training_set_size)[features].copy()\n",
    "training_labels = randomized_data.head(training_set_size)[labels].copy()\n",
    "print(training_features.head())\n",
    "print(training_labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to define our feature columns for TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(key) for key in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>6. Select ML Algorithm and Train</b></u></font>\n",
    "\n",
    "There are many models that can be used to attempt to solve the problem of classifying wheter the cancer is benign or malignant. In this example, we will use a neural network; which is a mathematical model that is inspired by how brains use.\n",
    "\n",
    "The strength of neural networks has been shown in the ability of these algorithms to excel in certain problems, especially classification. In the case of this problem, there is a deep pattern that is inside the set of data and the cancer outcome (otherwise, how would the physician's determination be better than a random determination). It seems like a fruitiful approach to develop neural network to classify each patient's data in terms of malignant or benign. \n",
    "\n",
    "<font size=4 color=B3A369><b>Neural Networks</b></font>\n",
    "\n",
    "Neural networks are a type of machine learning algorithm that are inspired by neurons in the human brain. Similar to neurons in the brains, neural networks are formed by interconnecting neurons that interact with each other. Each neuron takes input, does some simple alogrithm to it, and then passes an output to the next neuron.\n",
    "\n",
    "Let us look at a perceptron; that is, a single layer neural network. \n",
    "\n",
    "The *perceptron* is a mathematical function that takes a set of inputs, performs some operation, and outputs the result. In this case,\n",
    "$$ y = \\sum_{i} w_{i}x{i} + w_0,$$\n",
    "where $w_i$ is the weight of the perceptron and $w_0$ is the bias. Note that this is the form of a line (plane,hyperplane,...) The weights are used to determine the importance of the of that component and the bias shifts the activation function curve up and down. \n",
    "\n",
    "The results of the perceptron acting on the inputs, will be input into the activation function, which will determine how to classify the set. \n",
    "\n",
    "<font size=4 color=B3A369><b>Architecture of Neural Networks</b></font>\n",
    "\n",
    "A neural network consists of \n",
    "* An input layer \n",
    "* Any number of hidden layers (these are called hidden because the external observe does not see the output)\n",
    "* An output layer\n",
    "* A set of weights and bias between each layer $\\{w_i\\}, \\{b_i\\}$\n",
    "* An activation function for each layer, $\\sigma$\n",
    "\n",
    "<img src='image/neural_network_1.png'>\n",
    "\n",
    "<font size=4 color=B3A369><b>Training Process</b></font>\n",
    "\n",
    "Each iteration of the training process consists of the following steps:\n",
    "1. Calculating the predicted output $\\hat{y}$, known as _*Feedforward*_\n",
    "2. Updating the weights and biases, known as _*Backpropagation*_\n",
    "\n",
    "Schematicially, this can be illustated as \n",
    "<img src='image/nn_iteration.png'>\n",
    "\n",
    "<font size=3 color=B3A369><b>Feedforward</b></font>\n",
    "\n",
    "The forward motion is quite simply the calculation of the function in series, that is the the sum of the products of the weights and activations that lead to the neuron. Swe are moving forward in the network. \n",
    "\n",
    "The loss function comes into play at this point, since we must determine the \"goodness\" of our performance.\n",
    "There are many possibilities to use for the *loss* function, such as the familar *sum-of-squares error*\n",
    "$$ \\mathrm{loss} = \\sum_{i=1}^n (y-\\hat{y})^2$$\n",
    "\n",
    "<font size=3 color=B3A369><b>Backpropagation</b></font>\n",
    "\n",
    "As we measure the error of our prediction, we can now find a way to use the error to improve the network, if desired. This is termed *backpropagation*. We work away back to update the weights and biases for the neurons. \n",
    "\n",
    "Minimization of the error function is how this optimization. There are multiple methods to optimize these multiple dimension functions, a popular one method may be to use the derviative of the loss function to determine the path of greatest decrease as in *gradient descent*.\n",
    "\n",
    "<font size=4 color=B3A369><b>Hyperparameters</b></font>\n",
    "\n",
    "*Hyperparameters* are the *variables which determine the network structure* and *how the network is trained*. Examples that effect the *learning rate* are *epoch*, *batches*, and *iterations*. These are important parameters that are not learned by the network so they must be specified by the model designer. \n",
    "\n",
    "An *epoch* is when an entire training dataset is passed forward and backward through the network *once*. It is at the end of an epoch that parameters (weights and biases) have updated. In short (batch_size * number_iterations >= number_data)\n",
    "\n",
    "An *iteration* is the number of *batches* needed to complete one epoch.\n",
    "\n",
    "In some cases, the dataset will need to be divided into *batches* in order to fit everything in memory in order complete the calculations. Many ML frameworks natively support this with batches, but sometimes you may have to manually specify them.\n",
    "\n",
    "<font size=4 color=B3A369><b>Lets Try it Out</b></font>\n",
    "\n",
    "For our initial attempt, lets define a DNN Classifier with 4 layers, and hidden nodes determined as $$round((2*nodes_{n-1})/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,hidden_units=[12,10,9,8], n_classes=2,model_dir='tmp/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=B3A369><b>Train the Network</b></font>\n",
    "\n",
    "We define the training the input function now. \n",
    "\n",
    "The function that does this is \n",
    "\n",
    "`train_input_fn = tf.estimator.inputs.pandas_input_fn(x=training_features, y=training_labels['target'], num_epochs=15,shuffle=True)`\n",
    "\n",
    "In this case, we will pass through the data set 15 times, updating the weight and biases based on the loss.\n",
    "<https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn> for complete documentation of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x=training_features,y=training_labels['target'],num_epochs=15,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(training_features['mean_radius']), type(training_labels['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note** If you are reruning the calculation, it may be necessary to clean out the tmp directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn,steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=B3A369><u><b>7. Test Model</b></u></font>\n",
    "\n",
    "Now that we've trained our model, let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x=testing_features,y=testing_labels['target'],num_epochs=15,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy wasn't particularly high - and in fact, due to the stochastic nature of the algorithm, we can rerun it and get wildly different results (I've seen accuracy ranging from 50-95% accuracy!).\n",
    "\n",
    "So how do we improve our model? There are multiple approaches we can consider:\n",
    "- Increase hidden layers\n",
    "- Change activiation function\n",
    "- Change activation function in output layer\n",
    "- Increase number of neurons\n",
    "- Weight initialization\n",
    "- More data\n",
    "- Normalization/scaling data\n",
    "- Change learning algorithm parameters\n",
    "- Change our algorithm\n",
    "\n",
    "As an example, let's say we simply want to change algorithms and consider something simpler - for classification such as this, linear regression is often a great starting point. (Note - a neural network is extremely robust, and can actually achieve higher overall accuracy, but given the small dataset, there are multiple knobs to tune here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = tf.estimator.LinearClassifier(feature_columns=feature_columns,n_classes=2,model_dir='tmp/model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2.train(input_fn=train_input_fn,steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the accuracy improved, and consistently so - this is likely due to a variety of reasons, including:\n",
    "- small data size (more susceptible to random behavior)\n",
    "- less than optimal features (there were other, more highly correlated features)\n",
    "- naive NN design (hidden layers, number neurons, etc.)\n",
    "\n",
    "Again, this was less about the theory of ML and more about the tools, so we can still appreciate those benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Please provide feedback!](https://gatech.co1.qualtrics.com/jfe/form/SV_55uzMYLufTuiLch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "TODO:\n",
    "1) Run all cells and follow through\n",
    "\n",
    "2) Move Data split above scaling and exploration. Do exploration only on the training set\n",
    "\n",
    "3) Compare sections with other notebooks and borrow ideas\n",
    "\n",
    "4) Clear all outputs and save notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:app-of-ml]",
   "language": "python",
   "name": "conda-env-app-of-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
